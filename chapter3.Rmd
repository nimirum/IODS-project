# Week 3: Logistic regresion

This week's topic was about logistic regression

### The data

I read the students performance and alcoholic consumption data into R.

```{r warning=FALSE, message=FALSE}
library(tidyr); library(dplyr); library(ggplot2); library(boot)
alc <- read.csv('data/students_alc2014.csv',sep=',')
```

[The data set](https://archive.ics.uci.edu/ml/datasets/Student+Performance) is from UCI Machine Learning Repository, Student Performance Data Set and it  contains students performance in mathematics and portuguese, including alcohol consumption. The acl data is a joined data frame from mathematics and portuguese data. The data set has meen modified so that the 'alc_use' is the average of 'Dalc' and 'Walc'. The variable 'high_use' is TRUE if 'alc_use' is higher than 2 and FALSE otherwise. 

Glimpse of data and first 5 rows of it.

```{r}
glimpse(alc)
head(alc, n=5)
```

### Hypothesis testing
I choose 4 variables and present a hypothesis on how they correlate with high/low alchohol consumption. 
The variables are failure, sex, activities and absences from school. Below are my hypothesis:

H1: High alcohol consumption realtes to the amount of course failures.

H2: Sex indicates alcohol consumption. 

H3: More time spent on activities implies less alcohol consupmtion 

H4: High amount of absences relates to high alcohol consumption.


Below is the data structure. 

```{r warning=FALSE}
variables <- c("high_use","failures","activities","sex","absences")
alc_hyp_test <- select(alc, one_of(variables))
glimpse(alc_hyp_test)

gather(alc_hyp_test) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar(fill="darkslategray4")


g1 <- ggplot(alc, aes(x = failures, fill=high_use))
g1 + geom_bar() + ggtitle("Student failures and alcohol consumption")

g2 <- ggplot(alc, aes(x = sex, fill=high_use))
g2 + geom_bar() + ggtitle("Student sex and alcohol consumption")

g3 <- ggplot(alc, aes(x = activities, fill=high_use))
g3 + geom_bar() + ggtitle("Student activities and alcohol consumption")

g4 <- ggplot(alc, aes(x = high_use, y = absences, fill=high_use))
g4 + geom_boxplot() + ylab("absences") + ggtitle("Student absences by alcohol consumption and sex")

```

About 1/3 of students are categorized as high alcohol consumption users.
In first chart (g1) it is hard to say if high alchohol use affect failures or not.
The second chart (g2) indicates that males consumpt more alcohol than women.
In the third chart (g3) seems that activities implies less achohol use, but not that much.
In the last chart (g4) it looks like absences are related to high alchohol consumption.

### Logistic regression

```{r}
model <- glm(high_use ~ failures + sex + absences + activities -1, data = alc_hyp_test, family = "binomial")

summary(model)
```

The students high alchohol use seems to relate most with sex and amount of absences because the coefficents are ***. I drop activities and reproduce logistic regression without it.

```{r}
model <- glm(high_use ~ failures + sex + absences, data = alc_hyp_test, family = "binomial")

summary(model)

OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
```
In here we can see that people with alcohol use are more likely to have high course failure count and high chance of having absences. In odds ratio model the high alcohol students are more probably males than females. These results support my hypothesis.

### Prediction

```{r}
probabilities <- predict(model, type = "response")

alc_hyp_test <- mutate(alc_hyp_test, probability = probabilities)
alc_hyp_test <- mutate(alc_hyp_test, prediction = probabilities > 0.5)
table(high_use = alc_hyp_test$high_use, prediction = probabilities > 0.5)

g <- ggplot(alc_hyp_test, aes(x = probability, y = high_use, col= prediction))
g + geom_point()

table(high_use = alc_hyp_test$high_use, prediction = alc_hyp_test$prediction) %>% prop.table()  %>% addmargins()

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

loss_func(class = alc_hyp_test$high_use, prob = alc_hyp_test$probability)


```

The prediction model predicts about 24% of the predictions wrong.

### Cross validation

```{r}
cv <- cv.glm(data = alc_hyp_test, cost = loss_func, glmfit = model, K = 10)

cv$delta[1]

```

Cross validation (k=10) seems to add the amount of wrong predictions